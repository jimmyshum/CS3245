Q1. Yes, removing the number is OK. It is because getting a list of document with a number 
“314159” is not so meaningful. There are infinite amount of number can be indexed into the 
dictionary. So if they are removed, the dictionary size would be reduced greatly. 

The way to normalize number is that keep numbers smaller than 2099(can be years), numbers 
having 8 digits (can be phone number), normalize small number (1-100) to the english word. 
For example 1 normalize to one, 10 normalize to ten. 
Using this method, the rare large number are not indexed and the “useful” number are kept

The percentage of reduction is not calculated because there are not enough time to re-index
 the whole reuters collection.


Q2. After removing stop words, the dictionary size decrease. The posting list size will
 decrease greatly. The reason is that stop word appears a lot and it have long posting 
list. If they are removed, size of posting list decrease a lot. 

For the search result, because searching stop word will return almost all of the documents,
 a search query of phase having stop word will return a small set of result. 
E.g “the apple” will return result of “apple”. Results of word “the” are ignored. 
However, another effect of removing stop word’s data is that if user really want to search 
stop word, there will be no result. It is good for phrasal query only, not for single term
 query.


Q3 Take the example of "Yes. I am. Fine.". For sent_tokenize(), the tokenized list will become ['Yes.','I am.','Fine.']. While for word_tokenize(), the tokenized list will become ['Yes.','I','am.','Fine.']. From this example, it shows that sent_tokenize() and word_tokenize()  have different style of tokenization. 

Therefore, for further refining the result, we can first use the sent_tokenize() to tokenize the whole sentence and then after we can further break the token which still has the spaces.

